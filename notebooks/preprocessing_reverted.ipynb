{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f48a906",
   "metadata": {},
   "source": [
    "## Preprocessing of Data and Augmentation\n",
    "- smote_\"name of file\" e.g. smote_C4M1 (type = numpy array) for normalized data augmentation with SMOTE (Synthetic Minority Oversampling Technique)\n",
    "- gauss_data_\"name of file\" e.g gauss_data_C3M2 (type = numpy array) for normaliezed data augmentation with gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e54e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d973005",
   "metadata": {},
   "source": [
    "## Read in Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121becf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/Exercises_SS22/sleeplab_dataset_10hz/patient_29_male_7_years',\n",
       " '../data/Exercises_SS22/sleeplab_dataset_10hz/patient_75_female_5_years',\n",
       " '../data/Exercises_SS22/sleeplab_dataset_10hz/patient_80_female_5_years',\n",
       " '../data/Exercises_SS22/sleeplab_dataset_10hz/patient_89_female_6_years',\n",
       " '../data/Exercises_SS22/sleeplab_dataset_10hz/patient_91_female_7_years']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirname = \"../data/Exercises_SS22/sleeplab_dataset_10hz\"\n",
    "folders = []\n",
    "\n",
    "for folder in os.listdir(dirname):\n",
    "    f = os.path.join(dirname, folder)\n",
    "    x = f.replace('\\\\', '/')\n",
    "    folders.append(x)\n",
    "\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5bf82ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "readings = []\n",
    "names = []\n",
    "\n",
    "for filename in os.listdir(folders[0]):\n",
    "    f = os.path.join(folders[0], filename)\n",
    "    x = f.replace('\\\\', '/')\n",
    "    readings.append(x)\n",
    "    f = filename.replace('.csv','')\n",
    "    names.append(f)\n",
    "    \n",
    "measurements = [pd.read_csv(i, skiprows=1, names=[names[ix]]) for ix, i in enumerate(readings[:-1])]\n",
    "label = pd.read_csv(readings[-1], usecols=['Schlafstadium'])\n",
    "data = pd.concat(measurements, axis=1)\n",
    "converted_label = label.replace(['WK', 'REM', 'N1', 'N2', 'N3'], [0, 1, 2, 3, 4])\n",
    "normalized_df=(data-data.mean())/data.std()\n",
    "segments = np.array([[i] * 300 for i in range(len(converted_label))]).flatten()[:normalized_df.shape[0]]\n",
    "\n",
    "tuples = list(zip(segments, normalized_df.index))\n",
    "\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=[\"Samples\", \"Datapoints\"])\n",
    "multi_index_df = normalized_df.set_index(index)\n",
    "counted = multi_index_df.groupby(level=0).count()\n",
    "smallSampleIndices = counted.loc[counted.BeinLi_10HZ < 300].index\n",
    "if len(smallSampleIndices) > 0:\n",
    "    multi_index_df = multi_index_df.drop(smallSampleIndices)\n",
    "    converted_label = converted_label.drop(smallSampleIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3e0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "readings_2 = []\n",
    "names_2 = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(folders[1]):\n",
    "    f = os.path.join(folders[1], filename)\n",
    "    x = f.replace('\\\\', '/')\n",
    "    readings_2.append(x)\n",
    "    f = filename.replace('.csv','')\n",
    "    names_2.append(f)\n",
    "    # create single df\n",
    "    \n",
    "#read data\n",
    "measurements_2 = [pd.read_csv(i, skiprows=1, names=[names_2[ix]]) for ix, i in enumerate(readings_2[:-1])]\n",
    "label_2 = pd.read_csv(readings_2[-1], usecols=['Schlafstadium'])\n",
    "label_2.index += 1091\n",
    "# concat all single files\n",
    "data_2 = pd.concat(measurements_2, axis=1)\n",
    "# convert labels to ints\n",
    "converted_label_2 = label_2.replace(['WK', 'REM', 'N1', 'N2', 'N3'], [0, 1, 2, 3, 4])\n",
    "# normalize the data, because some paper said to do so\n",
    "normalized_df_2 = (data_2-data_2.mean())/data_2.std()\n",
    "segments_2 = np.array([[i] * 300 for i in range(len(converted_label_2))]).flatten()[:normalized_df_2.shape[0]]\n",
    "segments_2 += 1091\n",
    "normalized_df_2.index += 1091\n",
    "# create multi index dataframe\n",
    "tuples_2 = list(zip(segments_2, normalized_df_2.index))\n",
    "\n",
    "index_2 = pd.MultiIndex.from_tuples(tuples_2, names=[\"Samples\", \"Datapoints\"])\n",
    "multi_index_df_2 = normalized_df_2.set_index(index_2)\n",
    "# drop the sample, that isn't big enough\n",
    "counted_2 = multi_index_df_2.groupby(level=0).count()\n",
    "smallSampleIndices_2 = counted_2.loc[counted_2.BeinLi_10HZ < 300].index\n",
    "if len(smallSampleIndices_2) > 0:\n",
    "    multi_index_df_2 = multi_index_df_2.drop(smallSampleIndices_2)\n",
    "    converted_label_2 = converted_label_2.drop(smallSampleIndices_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192f66ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_frame = pd.concat([multi_index_df, multi_index_df_2])\n",
    "double_label = pd.concat([converted_label, converted_label_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d0433c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>BeinLi_10HZ</th>\n",
       "      <th>BeinRe_10HZ</th>\n",
       "      <th>C3M2_10HZ</th>\n",
       "      <th>C4M1_10HZ</th>\n",
       "      <th>EMG_10HZ</th>\n",
       "      <th>F3M2_10HZ</th>\n",
       "      <th>F4M1_10HZ</th>\n",
       "      <th>LEOGM2_10HZ</th>\n",
       "      <th>O1M2_10HZ</th>\n",
       "      <th>O2M1_10HZ</th>\n",
       "      <th>REOGM1_10HZ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Samples</th>\n",
       "      <th>Datapoints</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.106808</td>\n",
       "      <td>-0.115749</td>\n",
       "      <td>0.019222</td>\n",
       "      <td>-0.078712</td>\n",
       "      <td>0.105964</td>\n",
       "      <td>0.129148</td>\n",
       "      <td>-0.346040</td>\n",
       "      <td>0.153007</td>\n",
       "      <td>0.635409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.106808</td>\n",
       "      <td>-0.115749</td>\n",
       "      <td>0.019222</td>\n",
       "      <td>-0.078712</td>\n",
       "      <td>0.105964</td>\n",
       "      <td>0.129148</td>\n",
       "      <td>-0.346040</td>\n",
       "      <td>0.153007</td>\n",
       "      <td>0.635409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.106808</td>\n",
       "      <td>-0.115749</td>\n",
       "      <td>0.019222</td>\n",
       "      <td>-0.078712</td>\n",
       "      <td>0.105964</td>\n",
       "      <td>0.129148</td>\n",
       "      <td>-0.346040</td>\n",
       "      <td>0.153007</td>\n",
       "      <td>0.635409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.106808</td>\n",
       "      <td>-0.115749</td>\n",
       "      <td>0.019222</td>\n",
       "      <td>-0.078712</td>\n",
       "      <td>0.105964</td>\n",
       "      <td>0.129148</td>\n",
       "      <td>-0.346040</td>\n",
       "      <td>0.153007</td>\n",
       "      <td>0.635409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.106808</td>\n",
       "      <td>-0.115749</td>\n",
       "      <td>0.019222</td>\n",
       "      <td>-0.078712</td>\n",
       "      <td>0.105964</td>\n",
       "      <td>0.129148</td>\n",
       "      <td>-0.346040</td>\n",
       "      <td>0.153007</td>\n",
       "      <td>0.635409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2311</th>\n",
       "      <th>367386</th>\n",
       "      <td>0.335723</td>\n",
       "      <td>-1.267304</td>\n",
       "      <td>-0.064566</td>\n",
       "      <td>0.185251</td>\n",
       "      <td>2.523777</td>\n",
       "      <td>0.040725</td>\n",
       "      <td>0.281393</td>\n",
       "      <td>0.362348</td>\n",
       "      <td>0.519440</td>\n",
       "      <td>0.055299</td>\n",
       "      <td>-0.367360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367387</th>\n",
       "      <td>-0.424523</td>\n",
       "      <td>-0.088911</td>\n",
       "      <td>-0.162140</td>\n",
       "      <td>0.092491</td>\n",
       "      <td>0.184402</td>\n",
       "      <td>-0.252190</td>\n",
       "      <td>-0.585706</td>\n",
       "      <td>-0.042996</td>\n",
       "      <td>0.500783</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>-0.866948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367388</th>\n",
       "      <td>0.552936</td>\n",
       "      <td>-0.179556</td>\n",
       "      <td>-0.045051</td>\n",
       "      <td>-0.031190</td>\n",
       "      <td>-0.149794</td>\n",
       "      <td>-0.159690</td>\n",
       "      <td>-0.441189</td>\n",
       "      <td>0.030703</td>\n",
       "      <td>0.407496</td>\n",
       "      <td>0.246723</td>\n",
       "      <td>-0.577713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367389</th>\n",
       "      <td>0.227116</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>-0.493892</td>\n",
       "      <td>-0.123951</td>\n",
       "      <td>-1.319482</td>\n",
       "      <td>-0.329273</td>\n",
       "      <td>-0.812803</td>\n",
       "      <td>-0.208818</td>\n",
       "      <td>0.202264</td>\n",
       "      <td>-0.327549</td>\n",
       "      <td>-0.682889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367390</th>\n",
       "      <td>0.118510</td>\n",
       "      <td>-0.179556</td>\n",
       "      <td>-0.337773</td>\n",
       "      <td>0.053841</td>\n",
       "      <td>-0.316893</td>\n",
       "      <td>-0.128857</td>\n",
       "      <td>0.095586</td>\n",
       "      <td>-0.190394</td>\n",
       "      <td>-0.282829</td>\n",
       "      <td>0.274069</td>\n",
       "      <td>0.263699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>693600 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BeinLi_10HZ  BeinRe_10HZ  C3M2_10HZ  C4M1_10HZ  EMG_10HZ  \\\n",
       "Samples Datapoints                                                             \n",
       "0       0              0.013138     0.008852   0.106808  -0.115749  0.019222   \n",
       "        1              0.013138     0.008852   0.106808  -0.115749  0.019222   \n",
       "        2              0.013138     0.008852   0.106808  -0.115749  0.019222   \n",
       "        3              0.013138     0.008852   0.106808  -0.115749  0.019222   \n",
       "        4              0.013138     0.008852   0.106808  -0.115749  0.019222   \n",
       "...                         ...          ...        ...        ...       ...   \n",
       "2311    367386         0.335723    -1.267304  -0.064566   0.185251  2.523777   \n",
       "        367387        -0.424523    -0.088911  -0.162140   0.092491  0.184402   \n",
       "        367388         0.552936    -0.179556  -0.045051  -0.031190 -0.149794   \n",
       "        367389         0.227116     0.001735  -0.493892  -0.123951 -1.319482   \n",
       "        367390         0.118510    -0.179556  -0.337773   0.053841 -0.316893   \n",
       "\n",
       "                    F3M2_10HZ  F4M1_10HZ  LEOGM2_10HZ  O1M2_10HZ  O2M1_10HZ  \\\n",
       "Samples Datapoints                                                            \n",
       "0       0           -0.078712   0.105964     0.129148  -0.346040   0.153007   \n",
       "        1           -0.078712   0.105964     0.129148  -0.346040   0.153007   \n",
       "        2           -0.078712   0.105964     0.129148  -0.346040   0.153007   \n",
       "        3           -0.078712   0.105964     0.129148  -0.346040   0.153007   \n",
       "        4           -0.078712   0.105964     0.129148  -0.346040   0.153007   \n",
       "...                       ...        ...          ...        ...        ...   \n",
       "2311    367386       0.040725   0.281393     0.362348   0.519440   0.055299   \n",
       "        367387      -0.252190  -0.585706    -0.042996   0.500783   0.000607   \n",
       "        367388      -0.159690  -0.441189     0.030703   0.407496   0.246723   \n",
       "        367389      -0.329273  -0.812803    -0.208818   0.202264  -0.327549   \n",
       "        367390      -0.128857   0.095586    -0.190394  -0.282829   0.274069   \n",
       "\n",
       "                    REOGM1_10HZ  \n",
       "Samples Datapoints               \n",
       "0       0              0.635409  \n",
       "        1              0.635409  \n",
       "        2              0.635409  \n",
       "        3              0.635409  \n",
       "        4              0.635409  \n",
       "...                         ...  \n",
       "2311    367386        -0.367360  \n",
       "        367387        -0.866948  \n",
       "        367388        -0.577713  \n",
       "        367389        -0.682889  \n",
       "        367390         0.263699  \n",
       "\n",
       "[693600 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b8fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07d1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ix, test_ix = train_test_split(double_frame.index.levels[0][:-1], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd7466f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_X = double_frame.loc[train_ix]\n",
    "train_y = double_label.loc[train_ix]\n",
    "test_X = double_frame.loc[test_ix]\n",
    "test_y = double_label.loc[test_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed38b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, label_df):\n",
    "        self.dataframe = dataframe\n",
    "        self.label_df = label_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        output = torch.tensor(self.dataframe.loc[list(set(self.dataframe.reset_index(0).Samples))[idx]].values.astype(np.float32)).unsqueeze(0)\n",
    "        label = self.label_df.loc[list(set(self.dataframe.reset_index(0).Samples))[idx]].values[0]\n",
    "        return output, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a120441",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_X, train_y)\n",
    "test_dataset = CustomDataset(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03dab204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2f255653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepClassification(ClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Conv2d(1, 20, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(20),\n",
    "            nn.Conv2d(20, 40, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        \n",
    "            nn.Conv2d(40, 80, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(80),\n",
    "            nn.Conv2d(80 ,80, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(80, 120, kernel_size = 2, stride = 1, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(120),\n",
    "            nn.Conv2d(120,120, kernel_size = 2, stride = 1, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(120, 160, kernel_size = 2, stride = 1, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(160),\n",
    "            nn.Conv2d(160,160, kernel_size = 2, stride = 1, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(160, 240, kernel_size = 2, stride = 1, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(240),\n",
    "            nn.Conv2d(240,240, kernel_size = 2, stride = 1, padding = 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(5280, 5),\n",
    "            nn.Sigmoid()\n",
    "            # nn.Dense(5, activation='sigmoid')\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "979f12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59881f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 8.2556, val_loss: 8.1274, val_acc: 0.2467\n",
      "Epoch [1], train_loss: 8.0982, val_loss: 8.0877, val_acc: 0.3240\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = SleepClassification()\n",
    "\n",
    "num_epochs = 2\n",
    "opt_func = torch.optim.RAdam\n",
    "lr = 0.001#fitting the model on training data and record the result after each epoch\n",
    "history = fit(num_epochs, lr, model, train_dataloader, test_dataloader, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eea37d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gaussian noise\n",
    "mu, sigma = 0, 0.1\n",
    "noise = np.random.normal(mu, sigma, normalized_df_2.shape) \n",
    "\n",
    "converted_label_noise = converted_label_2 \n",
    "gauss_df = normalized_df_2 + noise\n",
    "\n",
    "segments_3 = np.array([[i] * 300 for i in range(len(converted_label_2))]).flatten()[:gauss_df.shape[0]]\n",
    "segments_3 += 2312\n",
    "gauss_df.index += 2312\n",
    "# create multi index dataframe\n",
    "tuples_3 = list(zip(segments_3, gauss_df.index))\n",
    "\n",
    "index_3 = pd.MultiIndex.from_tuples(tuples_3, names=[\"Samples\", \"Datapoints\"])\n",
    "multi_index_df_noise = gauss_df.set_index(index_3)\n",
    "# drop the sample, that isn't big enough\n",
    "counted_3 = multi_index_df_noise.groupby(level=0).count()\n",
    "smallSampleIndices_3 = counted_3.loc[counted_3.BeinLi_10HZ < 300].index\n",
    "if len(smallSampleIndices_3) > 0:\n",
    "    multi_index_df_noise = multi_index_df_noise.drop(smallSampleIndices_3)\n",
    "    converted_label_noise = converted_label_noise.drop(smallSampleIndices_3)\n",
    "\n",
    "\n",
    "\n",
    "aug_frame = pd.concat([multi_index_df, multi_index_df_2, multi_index_df_noise])\n",
    "aug_label = pd.concat([converted_label, converted_label_2, converted_label_noise])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44fa009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ix, vali_ix = train_test_split(double_frame.index.levels[0][:-1], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33e3f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = double_frame.loc[train_ix]\n",
    "train_y = double_label.loc[train_ix]\n",
    "vali_X = double_frame.loc[vali_ix]\n",
    "vali_y = double_label.loc[vali_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "933fcb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_aug = CustomDataset(train_X, train_y)\n",
    "vali_dataset_aug = CustomDataset(vali_X, vali_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7096e2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 1.3109, val_loss: 1.1440, val_acc: 0.7397\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader_aug = DataLoader(train_dataset_aug, batch_size=8, shuffle=True)\n",
    "vali_dataloader_aug = DataLoader(vali_dataset_aug, batch_size=8, shuffle=True)\n",
    "\n",
    "model = SleepClassification()\n",
    "\n",
    "num_epochs = 1\n",
    "opt_func = torch.optim.RAdam\n",
    "lr = 0.001#fitting the model on training data and record the result after each epoch\n",
    "history_aug = fit(num_epochs, lr, model, train_dataloader_aug, vali_dataloader_aug, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc59d1a",
   "metadata": {},
   "source": [
    "## Testing the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "46bc94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "readings = []\n",
    "names = []\n",
    "\n",
    "for filename in os.listdir(folders[3]):\n",
    "    f = os.path.join(folders[0], filename)\n",
    "    x = f.replace('\\\\', '/')\n",
    "    readings.append(x)\n",
    "    f = filename.replace('.csv','')\n",
    "    names.append(f)\n",
    "    \n",
    "measurements = [pd.read_csv(i, skiprows=1, names=[names[ix]]) for ix, i in enumerate(readings[:-1])]\n",
    "label = pd.read_csv(readings[-1], usecols=['Schlafstadium'])\n",
    "data = pd.concat(measurements, axis=1)\n",
    "converted_label = label.replace(['WK', 'REM', 'N1', 'N2', 'N3'], [0, 1, 2, 3, 4])\n",
    "normalized_df=(data-data.mean())/data.std()\n",
    "segments = np.array([[i] * 300 for i in range(len(converted_label))]).flatten()[:normalized_df.shape[0]]\n",
    "\n",
    "tuples = list(zip(segments, normalized_df.index))\n",
    "\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=[\"Samples\", \"Datapoints\"])\n",
    "multi_index_df = normalized_df.set_index(index)\n",
    "counted = multi_index_df.groupby(level=0).count()\n",
    "smallSampleIndices = counted.loc[counted.BeinLi_10HZ < 300].index\n",
    "if len(smallSampleIndices) > 0:\n",
    "    test_multi_index_df = multi_index_df.drop(smallSampleIndices)\n",
    "    test_converted_label = converted_label.drop(smallSampleIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f9e63947",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_multi_index_df, test_converted_label)\n",
    "\n",
    "modeloutput = []\n",
    "realoutput = []\n",
    "\n",
    "for i in range(20):\n",
    "    data, label = test_dataset.__getitem__(i)\n",
    "    modeloutput.append(model(data.unsqueeze(0)))\n",
    "    realoutput.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6cc65630",
   "metadata": {},
   "outputs": [],
   "source": [
    "realoutput = nn.functional.one_hot(torch.tensor(realoutput), num_classes = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0013d8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0500)\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.functional import hamming_distance\n",
    "\n",
    "results = []\n",
    "for i, target in enumerate(modeloutput):\n",
    "    results.append(hamming_distance(realoutput[i], target.to(torch.int64)))\n",
    "\n",
    "print(sum(results)/len(modeloutput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26102780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
