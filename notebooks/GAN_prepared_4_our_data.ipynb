{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2692147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a generative adversarial network on a one-dimensional function\n",
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ce9950",
   "metadata": {},
   "source": [
    "# DEFINE DISCRIMINATOR MODEL\n",
    "Diskriminator bekommt ein SAMPLE aus den echten Daten (hier aus den 200 echten Werten nur 2) und wirft wahr oder falsch aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8fd4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963e9036",
   "metadata": {},
   "source": [
    "# DEFINE GENERATOR MODEL\n",
    "Generator bekommt einen Punkt aus dem Latenten Raum undd generiert daraus ein neues Sample, bspw. Vektor mit input und output der Funktion (f(x) = x^2 dann x und y).\n",
    "\n",
    "The weights in the generator model are updated based on the performance of the discriminator model.\n",
    "\n",
    "Specifically, a new GAN model can be defined that stacks the generator and discriminator such that the generator receives as input random points in the latent space, generates samples that are fed into the discriminator model directly, classified, and the output of this larger model can be used to update the model weights of the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa352af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "    model.add(Dense(n_outputs, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238469ea",
   "metadata": {},
   "source": [
    "# DEFINE GAN MODEL INCLUDING GENERATOR AND DISCRIMINATOR\n",
    "Input: Point in latent space; Output: Binary classification (real, fake)\n",
    "\n",
    "The weights in the discriminator are marked as not trainable, which only affects the weights as seen by the GAN model and not the standalone discriminator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e1c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79a448",
   "metadata": {},
   "source": [
    "# LOAD REAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecfe16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n real samples with class labels\n",
    "def generate_real_samples(n):\n",
    "    # generate inputs in [-0.5, 0.5]\n",
    "    X1 = rand(n) - 0.5\n",
    "    # generate outputs X^2\n",
    "    X2 = X1 * X1\n",
    "    # stack arrays\n",
    "    X1 = X1.reshape(n, 1)\n",
    "    X2 = X2.reshape(n, 1)\n",
    "    X = hstack((X1, X2))\n",
    "    # generate class labels\n",
    "    y = ones((n, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629810ac",
   "metadata": {},
   "source": [
    "# TODO: LOAD OUR DATA AND BRING IT IN FORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3446605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(path_sensor, n):\n",
    "    # get number of rows\n",
    "    fs = sum(1 for line in open(path_sensor)) - 1\n",
    "    # get percentage from batchsize and filesize\n",
    "    p = 100 * n / fs\n",
    "    print('Percentage:', p)\n",
    "    # keep the header, then take only p% of lines\n",
    "    # if random from [0,1] interval is greater than 0.01 the row will be skipped\n",
    "    X1= pd.read_csv(\n",
    "             path_sensor,\n",
    "             header=0, \n",
    "             skiprows=lambda i: i>0 and random.random() > p\n",
    "    ).to_numpy()\n",
    "    X1 = X1[:,0]\n",
    "    # reshape data and c\n",
    "    X1 = X1.reshape(n, 1)\n",
    "    X2 = np.arange(n).reshape(n, 1)\n",
    "    print(X1.shape, X2.shape)\n",
    "    X = hstack((X1, X2))\n",
    "    # create label\n",
    "    y = np.ones((n, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2563488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(path_sensor, n):\n",
    "    #number of records in file (excludes header) fs->filesize\n",
    "    fs = sum(1 for line in open(path_sensor)) - 1\n",
    "    print('fs:', fs)\n",
    "    # get random start point\n",
    "    r = int(random.uniform(1, fs))\n",
    "    print('r:', r)\n",
    "    # check if start point has n numbers to the end\n",
    "    if fs-r>=n:\n",
    "        # read csv file skip r rows\n",
    "        X1 = pd.read_csv(path_sensor, skiprows=r, nrows=n).to_numpy()\n",
    "        X1 = X1[:,0]\n",
    "        # reshape data and c\n",
    "        X1 = X1.reshape(n, 1)\n",
    "        X2 = np.arange(n).reshape(n, 1)\n",
    "        print(X1.shape, X2.shape)\n",
    "        X = hstack((X1, X2))\n",
    "        # create label\n",
    "        y = np.ones((n, 1))\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec02d70",
   "metadata": {},
   "source": [
    "# n RANDOM WERTE AUS DEM LATENTEN RAUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e40d704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec6f0d0",
   "metadata": {},
   "source": [
    "# PREDICT SAMPLES USING GENERATOR & RANDOM LATENT SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfb5d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n)\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # create class labels\n",
    "    y = zeros((n, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0dff5",
   "metadata": {},
   "source": [
    "# EVALUATING PERFORMANCE\n",
    "1st Generate real samples\n",
    "\n",
    "2nd generate latent points\n",
    "\n",
    "3rd generate fake samples from 2nd\n",
    "\n",
    "4th plot both samples in one plot and compare\n",
    "\n",
    "Having both samples plotted on the same graph allows them to be directly compared to see if the same input and output domain are covered and whether the expected shape of the target function has been appropriately captured, at least subjectively.\n",
    "# SUMMARIZE PERFORMANCE\n",
    "The summarize_performance() function below can be called any time during training to create a scatter plot of real and generated points to get an idea of the current capability of the generator model.\n",
    "\n",
    "We may also be interested in the performance of the discriminator model at the same time.\n",
    "\n",
    "Specifically, we are interested to know how well the discriminator model can correctly identify real and fake samples. A good generator model should make the discriminator model confused, resulting in a classification accuracy closer to 50% on real and fake examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "255933cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the discriminator and plot real and fake points\n",
    "def summarize_performance_plot(epoch, generator, discriminator, latent_dim, path_sensor, n=100):\n",
    "    global accarr_real, accarr_fake\n",
    "    # prepare real samples\n",
    "    x_real, y_real = generate_real_samples(path_sensor, n)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    accarr_real.append(acc_real)\n",
    "    accarr_fake.append(acc_fake)\n",
    "    print('Epoch:', epoch, 'Accurracy real:', acc_real, 'Accurracy fake:', acc_fake)\n",
    "    # scatter plot real and fake data points\n",
    "    plt.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    "    plt.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9f66885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the discriminator and plot real and fake points\n",
    "def summarize_performance(epoch, generator, discriminator, latent_dim, path_sensor, n=100):\n",
    "    global accarr_real, accarr_fake\n",
    "    # prepare real samples\n",
    "    x_real, y_real = generate_real_samples(path_sensor, n)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    accarr_real.append(acc_real)\n",
    "    accarr_fake.append(acc_fake)\n",
    "    print('Epoch:', epoch, 'Accurracy real:', acc_real, 'Accurracy fake:', acc_fake)\n",
    "    # scatter plot real and fake data points\n",
    "    #plt.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    "    #plt.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d2ea9a",
   "metadata": {},
   "source": [
    "# TRAIN COMPOSITE MODEL\n",
    "Input: random points in latent space, Output: Binary Classification (real, fake)\n",
    "\n",
    "What is required is that we first update the discriminator model with real and fake samples, then update the generator via the composite model.\n",
    "\n",
    "This requires combining elements from the train_discriminator() function defined in the discriminator section and the train_gan() function defined above. It also requires that the generate_fake_samples() function use the generator model to generate fake samples instead of generating random numbers.\n",
    "\n",
    "Furthermore implement the evaluation functions every n_eval epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3db593d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, path_sensor, n_epochs=10000, n_batch=128, n_eval=2000):\n",
    "    global accarr_real, accarr_fake\n",
    "    # determine half the size of one batch, for updating the discriminator\n",
    "    half_batch = int(n_batch / 2)\n",
    "    print('train:', n_batch, half_batch)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        # prepare real samples\n",
    "        x_real, y_real = generate_real_samples(path_sensor, half_batch)\n",
    "        # prepare fake examples\n",
    "        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        # update discriminator\n",
    "        \n",
    "        d_model.train_on_batch(x_real, y_real)\n",
    "        d_model.train_on_batch(x_fake, y_fake)\n",
    "        \n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = ones((n_batch, 1))\n",
    "        \n",
    "        # update the generator via the discriminator's error\n",
    "        gan_model.train_on_batch(x_gan, y_gan)\n",
    "        \n",
    "        # plot evaluation of the model every n_eval epochs\n",
    "        if (i+1) % n_eval == 0:\n",
    "            summarize_performance_plot(i, g_model, d_model, path_sensor, latent_dim)\n",
    "        #evaluate the model every epoch\n",
    "        else:\n",
    "            summarize_performance(i, g_model, d_model, path_sensor, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da43cc7c",
   "metadata": {},
   "source": [
    "# __MAIN__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c44839a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "train: 256 128\n",
      "fs: 327310\n",
      "r: 65010\n",
      "(128, 1) (128, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 21:03:13.498900: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-06-30 21:03:13.499008: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-06-30 21:03:13.619297: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-06-30 21:03:13.650127: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-06-30 21:03:13.894980: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-06-30 21:03:14.137904: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fs: -1\n",
      "r: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m gan_model \u001b[38;5;241m=\u001b[39m define_gan(generator, discriminator)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgan_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_sensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(g_model, d_model, gan_model, latent_dim, path_sensor, n_epochs, n_batch, n_eval)\u001b[0m\n\u001b[1;32m     29\u001b[0m     summarize_performance_plot(i, g_model, d_model, path_sensor, latent_dim)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#evaluate the model every epoch\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[43msummarize_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_sensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36msummarize_performance\u001b[0;34m(epoch, generator, discriminator, latent_dim, path_sensor, n)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m accarr_real, accarr_fake\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# prepare real samples\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m x_real, y_real \u001b[38;5;241m=\u001b[39m generate_real_samples(path_sensor, n)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# evaluate discriminator on real examples\u001b[39;00m\n\u001b[1;32m      7\u001b[0m _, acc_real \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mevaluate(x_real, y_real, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 5\n",
    "# number of epochs\n",
    "n_epochs = 100\n",
    "# batchsize\n",
    "n_batch = 256\n",
    "# evaluate every n epoch\n",
    "n_eval = 10\n",
    "# accuracy array[2, epoch] -> 2: acc real, accfake -> epoch: number epochs\n",
    "accarr_real = []\n",
    "accarr_fake = []\n",
    "# path to sensor\n",
    "path_sensor = '../data/Exercises_SS22/sleeplab_dataset_10hz/patient_29_male_7_years/BeinLI_10HZ.csv'\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, latent_dim, path_sensor, n_epochs, n_batch, n_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accarr_real = np.asarray(accarr_real).reshape((len(accarr_real),1))\n",
    "accarr_fake = np.asarray(accarr_fake).reshape((len(accarr_fake),1))\n",
    "    \n",
    "plt.figure(figsize=(10, 7)) \n",
    "plt.legend()\n",
    "    \n",
    "plt.title('Development of the Accurracy of the Epochs')\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Accurracy\")\n",
    "    \n",
    "plt.plot(np.arange(n_epochs), accarr_real, color='red', label='acc_real')\n",
    "plt.plot(np.arange(n_epochs), accarr_fake, color='blue', label='acc_fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80ba3f",
   "metadata": {},
   "source": [
    "## sources:\n",
    "\n",
    "https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c897cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
